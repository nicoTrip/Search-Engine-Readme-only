# Distributed Search Engine

## Note
- Source code is not publicly available due to academic integrity policies.
- I’m happy to walk through the code upon request (for non-students only).

## Overview
This project involved building a scalable search engine that crawled over **1 million web pages**. We began with a basic, in-memory MapReduce framework developed during homeworks and expanded it into a **distributed system** with persistent storage, batched communication, and massive scalability.

The system included a crawler, TF-IDF pipeline, inverted indexer, PageRank, and a frontend that returned ranked pages based on user search queries.

## System Highlights
- Built a highly distributed system running across dozens of AWS EC2 instances
- Implemented a **sharded, persistent key-value store** to handle storage at scale
- Used **Java multithreading** to parallelize networking and speed up system throughput
- Supported key search engine components: **crawling**, **indexing**, **ranking**, and **query processing**
- Developed a web interface for:
  - Monitoring active crawls
  - Updating a blacklist to avoid low-value or problematic URLs

## My Contributions
I was responsible for:

- **Web Scraping**  
  Built a highly distributed crawler capable of downloading 1 million+ pages while respecting best crawling practices (robots.txt, domain throttling, etc.)

- **MapReduce Infrastructure**  
  Designed and optimized all stages of the data pipeline:
  - **TF-IDF Computation** – scored term importance per document
  - **Inverted Index Construction** – enabled fast keyword-based search
  - **PageRank Calculation** – assigned global importance scores based on link structure

- **Performance Optimization**  
  Tuned key aspects of the system, including:
  - Level of parallelism
  - File formats for intermediate storage
  - Communication batching strategies
  - System load balancing

I also helped with:

- **Cloud Deployment (AWS EC2)**  
  Helped deploy and monitor the full system on distributed EC2 instances for large-scale crawling and indexing.

- **Frontend**
  Worked on weighting all scores to return the best pages. Or weights included pagerank, TF-IDF score, and a phrasal search


## Technologies Used
- **Java** for all backend components
- **Custom-built MapReduce framework**
- **File I/O** for scalable storage
- **HTML parsing** (via [JSoup](https://jsoup.org))

## Challenges & What I Learned

### Scalability Lessons
Each order of magnitude in scale exposed new bottlenecks:
- At ~1,000 pages: in-memory storage broke down so we moved to file-based storage
- At ~10,000 pages: single-threaded networking became a bottleneck so we added multithreading and improved I/O patterns
- At ~100,000 pages: individual EC2 instances weren't sufficient so we redesigned the system to be fully distributed
- Every stage of the pipeline (crawl, index, rank) required different optimizations as the scale increased

### File I/O & Networking Bottlenecks
- Writing many small files was inefficient (due to 4kB disk page sizes), so we needed fewer, larger files
    - this also required us to have better chaching so we didn't need to read the entire file repeatedly
- Appending to files reduced write time, but required careful file management
- Sending one message per URL or task slowed the networking so we aggressively batchined all messages

### Web Crawling at Scale
- Could not crawl every link or visit most of the web so we had to prioritize and filter
- Many URLs were dead, slow, or spammy which lead us to implement filtering heuristics and URL blacklists
- Example blacklist rules:
  - Regex filters like `.*[0-9]{10,}.*` to avoid autogenerated URLs
  - Blocked non-English Wikipedia (`[a-z]*.wiki.*.org`) while whitelisting English
  - Avoided problematic sites like `help.imdb.com`


